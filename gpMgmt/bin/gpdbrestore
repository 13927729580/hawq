#!/usr/bin/env python
#
# Copyright (c) Greenplum Inc 2008. All Rights Reserved. 
#
#
# THIS IMPORT MUST COME FIRST
#
# import mainUtils FIRST to get python version check
from gppylib.mainUtils import *

import fnmatch
from optparse import Option, OptionGroup, OptionParser, OptionValueError

try:
    from tempfile import mkstemp

    from gppylib.gpparseopts import OptParser, OptChecker

    from gppylib import pgconf
    from gppylib import userinput
    from gppylib.commands.base import Command
    from gppylib.commands.gp import Scp
    from gppylib.commands.unix import Ping
    from gppylib.db import dbconn
    from gppylib.gparray import GpArray
    from gppylib import gplog

    from gppylib.operations import Operation
    from gppylib.operations.unix import CheckFile, CheckRemoteFile, ListFilesByPattern, ListRemoteFilesByPattern
    from gppylib.operations.utils import DEFAULT_NUM_WORKERS
    from gppylib.operations.restore import RestoreDatabase, RestoreTables, RecoverRemoteDumps, ValidateTimestamp, ValidateRestoreTables, GetDumpTables, GetDbName
    from gppylib.operations.restore import DUMP_DIR, DBDUMP_PREFIX, MASTER_DBDUMP_PREFIX, GLOBAL_PREFIX, CREATEDB_PREFIX

except ImportError, e:
    sys.exit('Cannot import modules.  Please check that you have sourced greenplum_path.sh.  Detail: ' + str(e))

# MPP-13617                                                                                                                                  
import re
RE1=re.compile('\\[([^]]+)\\]:(.+)')

WARN_MARK = '<<<<<'
logger = gplog.get_default_logger()

class GpdbRestore(Operation):
    def __init__(self, options, args):
        # only one of -t, -b, -R, and -s can be provided
        count = sum([1 for opt in ['db_timestamp', 'db_date_dir', 'db_host_path', 'search_for_dbname']
                       if options.__dict__[opt] is not None])
        if count == 0:
            raise ProgramArgumentValidationException("Must supply one of -t, -b, -R, or -s.")
        elif count > 1:
            raise ProgramArgumentValidationException("Only supply one of -t, -b, -R, or -s.")

        if options.list_tables and options.db_timestamp is None:
            raise ProgramArgumentValidationException("Need to supply -t <timestamp> for -L option")

        if options.masterDataDirectory is None:
            options.masterDataDirectory = gp.get_masterdatadir()

        self.db_timestamp = options.db_timestamp
        self.list_tables = options.list_tables
        self.db_date_dir = options.db_date_dir
        self.db_host_path = options.db_host_path
        self.search_for_dbname = options.search_for_dbname
        self.drop_db = options.drop_db
        self.restore_global = options.restore_global
        self.restore_tables = options.restore_tables
        self.batch_default = options.batch_default
        self.keep_dump_files = options.keep_dump_files
        self.no_analyze = options.no_analyze

        if self.restore_tables is not None:
            self.restore_tables = self.restore_tables.split(',')
        if self.db_host_path is not None:
            # MPP-13617                                                                                                                      
            m = re.match(RE1, self.db_host_path)
            if m:
                self.db_host_path = m.groups()
            else:
                self.db_host_path = self.db_host_path.split(':')

        self.interactive = options.interactive
        self.master_datadir = options.masterDataDirectory
        self.master_port = self._get_master_port(self.master_datadir)

        self.gparray = None

        self.ddboost = options.ddboost
        if self.ddboost:
            if self.db_host_path is not None:
                raise ExceptionNoStackTraceNeeded("-R cannot be used with DDBoost parameters.")

    def execute(self):
        if self.ddboost:
            cmd = Command('DDBoost sync', 'gpddboost --sync --dir=%s' % self.master_datadir)
            cmd.run(validateAfter = True)

        self.gparray = GpArray.initFromCatalog(dbconn.DbURL(port = self.master_port), utility=True)

        if self.list_tables and self.db_timestamp is not None:
            return self._list_dump_tables()

        info = self._gather_info()

        self._output_info(info)
        
        if self.interactive:
            if not userinput.ask_yesno(None, "\nContinue with Greenplum restore", 'N'):
                raise UserAbortedException()

        if self.db_host_path is not None:
            host, path = self.db_host_path
            RecoverRemoteDumps(host = host, path = path, 
                               restore_timestamp = info['restore_timestamp'],
                               compress = info['compress'],
                               restore_global = self.restore_global,
                               batch_default = self.batch_default,
                               master_datadir = self.master_datadir,
                               master_port = self.master_port).run()

        if self.restore_tables is not None:
            RestoreTables(restore_timestamp = info['restore_timestamp'],
                          restore_tables = self.restore_tables,
                          no_analyze = self.no_analyze,
                          keep_dump_files = self.keep_dump_files,
                          batch_default = self.batch_default,
                          master_datadir = self.master_datadir,
                          master_port = self.master_port,
                          ddboost = self.ddboost).run()
        else:
            RestoreDatabase(restore_timestamp = info['restore_timestamp'],
                            no_analyze = self.no_analyze,
                            drop_db = self.drop_db,
                            restore_global = self.restore_global,
                            master_datadir = self.master_datadir,
                            master_port = self.master_port,
                            ddboost = self.ddboost).run()

        if self.no_analyze:
            logger.warn('--------------------------------------------------------------------------------------------------')
            logger.warn('Analyze bypassed on request; database performance may be adversely impacted until analyze is done.')
            logger.warn('--------------------------------------------------------------------------------------------------')
    
    def _output_info(self, info):
        logger.info("------------------------------------------")
        logger.info("Greenplum database restore parameters")
        logger.info("------------------------------------------")
        if self.restore_tables is not None:
            logger.info("Restore type               = Table Restore")
            logger.info("Database name              = %s" % info['restore_db'])
            logger.info("------------------------------------------")
            logger.info("Table restore list")
            logger.info("------------------------------------------")
            for restore_table in self.restore_tables:       
                logger.info("Table                      = %s" % restore_table)
            if info['table_counts']:
                logger.info("------------------------------------------")
                logger.warn("Following tables have non-zero row counts %s" % WARN_MARK)
                logger.info("------------------------------------------")
                for table, count in info['table_counts']:
                    logger.warn("Table:Row count            = %s:%s" % (table, count))
                logger.info("------------------------------------------")
        else:
            logger.info("Restore type               = Full Database")
            logger.info("Database to be restored    = %s" % info['restore_db'])
            if self.drop_db:
                logger.info("Drop and re-create db      = On")
            else:
                logger.info("Drop and re-create db      = Off")        

        if self.db_timestamp is not None and self.restore_tables is None:
            logger.info("Restore method             = Restore specific timestamp")
        if self.search_for_dbname is not None and self.restore_tables is None:
            logger.info("Restore method             = Search for latest")
        if self.db_date_dir is not None and self.restore_tables is None:
            logger.info("Restore method             = Restore specific date")
        if self.restore_tables is not None:
            logger.info("Restore method             = Specific table restore")
        if self.db_host_path is not None:
            host, path = self.db_host_path
            logger.info("Restore method             = Remote host")
            logger.info("Recovery hostname          = %s" % host)
            logger.info("Remote recovery path       = %s" % path)
        logger.info("Restore timestamp          = %s" % info['restore_timestamp'])
        if info['compress']:
            logger.info("Restore compressed dump    = On")
        else:
            logger.info("Restore compressed dump    = Off")
        
        """
        TODO: These 3 lines are removed, b/c they pertain only to partial restore. 
        #logger.info("DBID List or restore       = %s" % dbid_list)
        #logger.info("Content list for restore   = %s" % content_list)
        #logger.info("Restore type               = %s" % restore_type)
        """

        if self.restore_global:
            logger.info("Restore global objects     = On")
        else:
            logger.info("Restore global objects     = Off")
        logger.info("Array fault tolerance      = %s" % info['fault_action'])

    def _get_master_port(self, datadir):
        """ TODO: This function will be widely used. Move it elsewhere?
            Its necessity is a direct consequence of allowing the -d <master_data_directory> option. From this,
            we need to deduce the proper port so that the GpArrays can be generated properly. """
        logger.debug("Obtaining master's port from master data directory")
        pgconf_dict = pgconf.readfile(datadir + "/postgresql.conf")
        return pgconf_dict.int('port')

    def _list_dump_tables(self):
        dump_tables = GetDumpTables(master_datadir = self.master_datadir,
                                    restore_timestamp = self.db_timestamp,
                                    ddboost = self.ddboost).run()

        logger.info("--------------------------------------------------------------------")
        logger.info("List of database tables for dump file with time stamp %s" % self.db_timestamp)
        logger.info("--------------------------------------------------------------------")
        for schema, table, owner in dump_tables:
            logger.info("Table %s.%s Owner %s" % (schema, table, owner))
        logger.info("--------------------------------------------------------------------")

    def _gather_info(self):
        fault_action = self.gparray.getFaultStrategy()
        primaries = [seg for seg in self.gparray.getDbList() if seg.isSegmentPrimary(current_role=True)]
        total_primary_segments = len(primaries)
        fail_count = len([seg for seg in primaries if seg.isSegmentDown()])

        if fault_action == 'readonly' and fail_count != 0:
            logger.fatal("There are %d primary segment databases marked as invalid")
            logger.fatal("Array fault action is set to readonly, unable to initiate a restore")
            logger.info("Use gprecoverseg utility to recover failed segment instances")
            raise ExceptionNoStackTraceNeeded("Unable to continue")

        (restore_timestamp, restore_db, compress) = (None, None, None)
        if self.db_timestamp is not None:
            (restore_timestamp, restore_db, compress) = ValidateTimestamp(master_datadir = self.master_datadir, 
                                                                          candidate_timestamp = self.db_timestamp).run()
        elif self.db_date_dir is not None:
            (restore_timestamp, restore_db, compress) = self._validate_db_date_dir()
        elif self.db_host_path is not None:
            (restore_timestamp, restore_db, compress) = self._validate_db_host_path()
        elif self.search_for_dbname is not None:
            (restore_timestamp, restore_db, compress) = self._search_for_latest()

        if not self.drop_db:
            dburl = dbconn.DbURL(port = self.master_port)
            conn = dbconn.connect(dburl)
            count = dbconn.execSQLForSingleton(conn, "select count(*) from pg_database where datname='%s';" % restore_db)             
            # TODO: -e is needed even if the database doesn't exist yet?
            if count == 0:
                raise ExceptionNoStackTraceNeeded("Database %s does not exist and -e option not supplied" % restore_db)

        table_counts = []
        if self.restore_tables is not None:
            (self.restore_tables, table_counts) = ValidateRestoreTables(restore_tables = self.restore_tables,
                                                                        restore_db = restore_db,
                                                                        master_port = self.master_port).run()

        return {'fault_action': fault_action,
                'fail_count': fail_count,
                'restore_timestamp': restore_timestamp,
                'restore_db': restore_db,
                'compress': compress,
                'table_counts': table_counts}
                 

    def _validate_db_date_dir(self):
        root = os.path.join(self.master_datadir, DUMP_DIR, self.db_date_dir)
        if not os.path.isdir(root):
            raise ExceptionNoStackTraceNeeded("Directory %s does not exist" % root)
        matching = ListFilesByPattern(root, "%s*" % CREATEDB_PREFIX).run()
        if len(matching) == 0:
            raise ExceptionNoStackTraceNeeded("Could not locate Master database dump files under %s" % root)
        if len(matching) > 1:
            dates_and_times = []
            for match in matching:
                temp = match[len(CREATEDB_PREFIX):]
                date, time = temp[0:8], temp[8:14]
                dates_and_times.append((date, time))
            restore_timestamp = self._select_multi_file(dates_and_times)
        else:
            match = matching[0]
            temp = match[len(CREATEDB_PREFIX):]
            restore_timestamp = temp[0:14]
        restore_db = GetDbName(os.path.join(root, "%s%s" % (CREATEDB_PREFIX, restore_timestamp))).run()
        compressed_file = "%s%s.gz" % (MASTER_DBDUMP_PREFIX, restore_timestamp)
        compress = CheckFile(os.path.join(root, compressed_file)).run()
        return (restore_timestamp, restore_db, compress)

    def _validate_db_host_path(self):
        # The format of the -R option should be 'hostname:path_to_dumpset', hence the length should be 2 (hostname, path_to_dumpset)
        if len(self.db_host_path) != 2:
            raise ProgramArgumentValidationException("The arguments of the -R flag are incorrect. The correct form should be as follows:\nIPv4_address:path_to_dumpset\n-OR-\n[IPv6_address]:path_to_dumpset\n", False)
        
        host, path = self.db_host_path       
        logger.debug("The host is %s" % host)
        logger.debug("The path is %s" % path)    

        Ping.local('Pinging %s' % host, host)
        matching = ListRemoteFilesByPattern(path, "%s*" % CREATEDB_PREFIX, host).run()
        if len(matching) == 0:
            raise ExceptionNoStackTraceNeeded("Could not locate Master database dump files at %s:%s" % (host,path))
        if len(matching) > 1:
            dates_and_times = []
            for match in matching:
                temp = match[len(CREATEDB_PREFIX):]
                date, time = temp[0:8], temp[8:14]
                dates_and_times.append((date, time))
            restore_timestamp = self._select_multi_file(dates_and_times)
        else:
            match = matching[0]
            temp = match[len(CREATEDB_PREFIX):]
            restore_timestamp = temp[0:14]

        # TODO: do a local GetDbName after Scp
        handle, filename = mkstemp() 
        srcFile = os.path.join(path, "%s%s" % (CREATEDB_PREFIX, restore_timestamp))
        Scp('Copying cdatabase file', srcHost=host, srcFile=srcFile, dstFile=filename).run(validateAfter=True)
        restore_db = GetDbName(filename).run()
        os.remove(filename)

        compressed_file = "%s%s.gz" % (MASTER_DBDUMP_PREFIX, restore_timestamp)
        compress = CheckRemoteFile(os.path.join(path, compressed_file), host).run() 
        return (restore_timestamp, restore_db, compress)
        
    def _search_for_latest(self):
        logger.info("Scanning Master host for latest dump file set for database %s" % self.search_for_dbname)
        root = os.path.join(self.master_datadir, DUMP_DIR)
        candidates, timestamps = [], []
        for path, dirs, files in os.walk(root):
            matching = fnmatch.filter(files, "%s*" % CREATEDB_PREFIX)
            candidates.extend([(path, filename) for filename in matching])
        if len(candidates) == 0:
            raise ExceptionNoStackTraceNeeded("No %s* files located" % CREATEDB_PREFIX)    
        for path, filename in candidates:
            db_name = GetDbName(os.path.join(path, filename)).run()
            if self.search_for_dbname == db_name:
                logger.info('Located dump file %s for database %s, adding to list' % (filename, self.search_for_dbname))
                timestamps.append(int(filename[len(CREATEDB_PREFIX):]))
            else:
                logger.info("Dump file has incorrect database name of %s, skipping..." % db_name)
        if len(timestamps) == 0:
            raise ExceptionNoStackTraceNeeded("No %s* files located with database %s" % (CREATEDB_PREFIX, self.search_for_dbname))
        restore_timestamp = str(max(timestamps)) 
        logger.info("Identified latest dump timestamp for %s as %s" %  (self.search_for_dbname, restore_timestamp))
        restore_db = self.search_for_dbname
        compressed_file = os.path.join(self.master_datadir, DUMP_DIR, restore_timestamp[0:8], "%s%s.gz" % (MASTER_DBDUMP_PREFIX, restore_timestamp))
        compress = CheckFile(compressed_file).run()
        return (restore_timestamp, restore_db, compress)
        
    def _select_multi_file(self, dates_and_times):
        spc = "        "
        def info(msg):
            print "%s%s" % (spc, msg)
        info("Select required dump file timestamp to restore")
        info(" #           Date    Time")
        info("--------------------------")
        for i in range(0, len(dates_and_times)):
            date, time = dates_and_times[i]
            info("[%d] ...... %s %s" % (i, date, time))
        info("Enter timestamp number to restore >")
        choice = raw_input()
        if choice == '':
            raise ExceptionNoStackTraceNeeded("Invalid or null input")
        if not choice.isdigit():
            raise ExceptionNoStackTraceNeeded("Invalid or null input")
        choice = int(choice)
        if choice < 0 or choice >= len(dates_and_times):
            raise ExceptionNoStackTraceNeeded("Invalid or null input")
        date, time = dates_and_times[choice]
        return "%s%s" % (date, time)

def create_parser():
    parser = OptParser(option_class=OptChecker, 
                       version='%prog version $Revision$',
                       description="Restores a Greenplum database or tables from a dumpset generated by gpdbdump")

    parser.setHelp([])                      # TODO: make gpparseopts do this
    addStandardLoggingAndHelpOptions(parser, includeNonInteractiveOption=True)

    addTo = OptionGroup(parser, 'Connection opts')
    parser.add_option_group(addTo)
    addMasterDirectoryOptionForSingleClusterProgram(addTo)

    addTo = OptionGroup(parser, 'Restore options: ')
    addTo.add_option('-t', dest='db_timestamp', metavar='<timestamp>', 
                     help="Timestamp key of backup file set that should be used for restore. Mandatory if -b, -R, -s are not supplied. Expects dumpset to reside on Greenplum array.")
    addTo.add_option('-L', action='store_true', dest='list_tables', default=False,
                     help="List table names in dump file, can only be used with -t <timestamp> option. Will display all tables in dump file, and then exit.")
    addTo.add_option('-b', dest='db_date_dir', metavar='<YYYYMMDD>',
                     help="%s/<YYYYMMDD> directory where dump files are located on the Greenplum array. Mandatory if -t, -s, -R options are not supplied." % DUMP_DIR)
    addTo.add_option('-R', dest='db_host_path', metavar='<hostname:dir_path>', 
                     help="Hostname and full directory path where backup set is located. Utility will recover files to master and segment hosts, and then start restore process.")
    addTo.add_option('-s', dest='search_for_dbname', metavar='<database name>', 
                     help="Search for latest backup set for the named database. Mandatory if -t, -b -R options are not supplied.")
    addTo.add_option('-e', action='store_true', dest='drop_db', default=False, 
                     help="Drop (erase) target database before recovery commences")
    addTo.add_option('-B', dest='batch_default', type='int', default=DEFAULT_NUM_WORKERS, metavar="<number>",
                     help="Dispatches work to segment hosts in batches of specified size [default: %s]" % DEFAULT_NUM_WORKERS)
    addTo.add_option('-G', action='store_true', dest='restore_global', default=False,
                     help="Restore global objects dump file if found in target dumpset. The global objects file is secured via the gpcrondump -G options, and has a filename of the form %s<timestamp>" % GLOBAL_PREFIX)
    addTo.add_option('-T', dest='restore_tables', metavar='<schema.tablename>', 
                     help="Restore a single table from a backup set. For multiple tables, supply a comma separated list of schema.tablenames. Note, table schema must exist in target database.")
    addTo.add_option('-K', action='store_true', dest='keep_dump_files', default=False,
                     help="Retain temporary generated table dump files.")
    addTo.add_option('--noanalyze', action='store_true', dest='no_analyze', default=False, 
                     help="Suppress the ANALYZE run following a successful restore. The user is responsible for running ANALYZE on any restored tables; failure to run ANALYZE following a restore may result in poor databse performance.")
    parser.add_option_group(addTo)

    ddOpt = OptionGroup(parser, "DDBoost")
    ddOpt.add_option('--ddboost', dest='ddboost', help="Dump to DDBoost using ~/.ddconfig", action="store_true", default=False)
    parser.add_option_group(ddOpt)

    return parser

if __name__ == '__main__':
    simple_main(create_parser, GpdbRestore)
